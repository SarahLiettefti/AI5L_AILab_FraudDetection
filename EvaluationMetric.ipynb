{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, log_loss, matthews_corrcoef\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://towardsdatascience.com/the-explanation-you-need-on-binary-classification-metrics-321d280b590f\n",
    "\n",
    "# Evaluation Metrics\n",
    "It is important to use the right metric to evaluate our models to have the right idea on its performance. In our case we are doing a binary classification. The target is either 1 or 0. In the dataste, there is 95469 non-fraudes observations for 193 fraudes. Because we are working with unbalanced dataset (their is much more non fraude data), there is a risk that the model could only predict 0 and have a good result. We have to find a way to prevent it. It is also important to not evaluat the prediction of a dataset that was also used in the training.\n",
    "\n",
    "Our goal as analysts is to contextualize and understand which metric offers us the most value. The metrics we're going to cover are : \n",
    "1. Accuracy : It tells the number of correct answer over the total number of answer. It is a common and simple metric but not recommenderd for our case of unbalanced data. Because if the model only predicts 0, it can have an accuracy of 99,7% while having failed to predict all the frauds. \n",
    "2. Precision : $TP \\over (TP+FP)$ It show how sensitive is the model to the signal to be recognized. So how often we are correct when we classify a class positive. More it is close to 1 and better it is. A high-precision model, means that it doesn't always recognize a fraude, but when it predicts it,  it is sure of it. \n",
    "3. Recall : $TP \\over (TP+FN)$ A model with a high recall will recognize as many positive classes as possible. We want a high recall if we want to be sure to detect all the fraud and don't care that sometimes it classifies non fraude as fraudes. (Includes noices)\n",
    "4. F1 score : combines precision and recall (2 complementary metrics) into one metric. $2 *$ $ {precision*recall} \\over {precision+recall}$ It is probably the most used metric for evaluating binary classification models. If our F1 score increases, it means that our model has increased performance for accuracy, recall or both.\n",
    "5. Log loss : This metric measures the difference between the probabilities of the modelâ€™s predictions and the probabilities of observed reality. The goal of this metric is to estimate the probability that an example has a positive class.\n",
    "6. Matthews Correlation Coefficient (MCC) : is designed to evaluate even models trained on unbalanced dataset. It ranges between $-1$ and $+1$. We want it to be near $+1$ since it indicates a correlation between actual observed values and predictions made by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(val_y, pred_y):\n",
    "    cm = confusion_matrix(val_y, pred_y)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metric = tp / (tp+fp)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(val_y, pred_y):\n",
    "    cm = confusion_matrix(val_y, pred_y)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metric = tp / (tp+fn)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(val_y, pred_y):\n",
    "    cm = confusion_matrix(val_y, pred_y)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    metric = 2*((precision*recall)/(precision+recall))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(val_y, pred_y):\n",
    "    metric = log_loss(val_y, pred_y)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(val_y, pred_y):\n",
    "    metric = matthews_corrcoef(val_y, pred_y)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(val_y, pred_y, model, description, csv = False):\n",
    "    precision = precision(val_y, pred_y)\n",
    "    recall = recall(val_y, pred_y)\n",
    "    f1score = f1score(val_y, pred_y)\n",
    "    logloss = logloss(val_y, pred_y)\n",
    "    mcc = mcc(val_y, pred_y)\n",
    "    d = {'Model': [model], 'Description': [description], 'Date':[pd.Timestamp.now()],\n",
    "         'Precision' : [precision], 'Recall' : [recall], 'F1-score' : [f1score], 'LogLoss' : [logloss], 'Mcc' : [mcc]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    if csv : \n",
    "        df.to_csv('evaluationmetric.csv', mode='a', header=False)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initreportcsv():\n",
    "    d = {'Model': [\"None\"], 'Description': ['Table init'], 'Date':[pd.Timestamp.now()],\n",
    "         'Precision' : [0], 'Recall' : [0], 'F1-score' : [0], 'LogLoss' : [0], 'Mcc' : [0]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv('evaluationmetric.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model Description                        Date  Precision  Recall  F1-score  LogLoss  Mcc\n",
      "0    NaN  Table init  2023-04-26 11:57:47.065184          0       0         0        0    0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Date</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Table init</td>\n",
       "      <td>2023-04-26 11:57:47.065184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Description                        Date  Precision  Recall  F1-score   \n",
       "0    NaN  Table init  2023-04-26 11:57:47.065184          0       0         0  \\\n",
       "\n",
       "   LogLoss  Mcc  \n",
       "0        0    0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def showreportcsv():\n",
    "    df = pd.read_csv('evaluationmetric.csv')\n",
    "    df.drop(\"Unnamed: 0\", axis=1, inplace = True)\n",
    "    print(df.to_string()) \n",
    "    return df\n",
    "showreportcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AILab-ECAM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
